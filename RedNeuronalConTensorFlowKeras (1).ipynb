{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " El volumen de la unidad C no tiene etiqueta.\n",
      " El número de serie del volumen es: F8C5-003D\n",
      "\n",
      " Directorio de C:\\Users\\Remigio\\Dropbox\\9.Docencia\\1.MATERIAS\\3.APRENDIZAJE DE MAQUINA\\2.Practicas\\3.PracticaRedesNeuronalesProblemasClasificacion\\CuadernosJupiter_Dataset\n",
      "\n",
      "15/04/2020  10:34    <DIR>          .\n",
      "15/04/2020  10:34    <DIR>          ..\n",
      "13/04/2020  11:36    <DIR>          .ipynb_checkpoints\n",
      "15/04/2020  01:09           349,061 RedNeuronalConTensorFlowKeras.html\n",
      "15/04/2020  01:09            54,798 RedNeuronalConTensorFlowKeras.ipynb\n",
      "15/04/2020  10:34             5,815 RedNeuronalSinLibrerias.ipynb\n",
      "02/04/2020  20:57    <DIR>          titanic\n",
      "07/10/2019  00:07            34,757 titanic.zip\n",
      "               4 archivos        444,431 bytes\n",
      "               4 dirs  923,140,694,016 bytes libres\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==2.0.0-beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>suicides_no</th>\n",
       "      <th>population</th>\n",
       "      <th>suicides/100k pop</th>\n",
       "      <th>country-year</th>\n",
       "      <th>HDI for year</th>\n",
       "      <th>gdp_for_year ($)</th>\n",
       "      <th>gdp_per_capita ($)</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>21</td>\n",
       "      <td>312900</td>\n",
       "      <td>6.71</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "      <td>796</td>\n",
       "      <td>Generation X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>35-54 years</td>\n",
       "      <td>16</td>\n",
       "      <td>308000</td>\n",
       "      <td>5.19</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "      <td>796</td>\n",
       "      <td>Silent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>female</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>14</td>\n",
       "      <td>289700</td>\n",
       "      <td>4.83</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "      <td>796</td>\n",
       "      <td>Generation X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>75+ years</td>\n",
       "      <td>1</td>\n",
       "      <td>21800</td>\n",
       "      <td>4.59</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "      <td>796</td>\n",
       "      <td>G.I. Generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34 years</td>\n",
       "      <td>9</td>\n",
       "      <td>274300</td>\n",
       "      <td>3.28</td>\n",
       "      <td>Albania1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "      <td>796</td>\n",
       "      <td>Boomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  year     sex          age  suicides_no  population  \\\n",
       "0  Albania  1987    male  15-24 years           21      312900   \n",
       "1  Albania  1987    male  35-54 years           16      308000   \n",
       "2  Albania  1987  female  15-24 years           14      289700   \n",
       "3  Albania  1987    male    75+ years            1       21800   \n",
       "4  Albania  1987    male  25-34 years            9      274300   \n",
       "\n",
       "   suicides/100k pop country-year  HDI for year  gdp_for_year ($)   \\\n",
       "0               6.71  Albania1987           NaN      2,156,624,900   \n",
       "1               5.19  Albania1987           NaN      2,156,624,900   \n",
       "2               4.83  Albania1987           NaN      2,156,624,900   \n",
       "3               4.59  Albania1987           NaN      2,156,624,900   \n",
       "4               3.28  Albania1987           NaN      2,156,624,900   \n",
       "\n",
       "   gdp_per_capita ($)       generation  \n",
       "0                 796     Generation X  \n",
       "1                 796           Silent  \n",
       "2                 796     Generation X  \n",
       "3                 796  G.I. Generation  \n",
       "4                 796          Boomers  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Esta guía explica el proceso para crear una arquitectura de redes neuronales, posteriormente explica el proceso de \n",
    "#optimización y fine tuning. Para ello, primeramente se define una red neuronal y se evalúa. Posteriormente, se ajustan\n",
    "#los parámetros y se evalúa el modelo final.\n",
    "\n",
    "#El dataset se llama Titanic, lo pueden encontrar en: \n",
    "#Dataset Kaggle: https://www.kaggle.com/c/titanic\n",
    "#También lo adjunto en el directorio de esta guía\n",
    "\n",
    "#Con este problema se intenta predecir si una persona sobrevivió o no sobrevivió, este tipo de problemas es aplicable a otros \n",
    "#problemas de sobrevivencia ante desastres.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('master.csv')\n",
    "df.head()\n",
    "\n",
    "#Aquí solo con la vista podemos descartar muchas variables que son las siguientes:\n",
    "#Nombre: No nos sirve porque no aporta información al resultado\n",
    "#Ticket: Tampoco nos aporta Información\n",
    "#Cabin: Podría aportarnos información pero tiene valores NaN (perdidos)\n",
    "#PassangerID: Es el indice, no aporta información relevante\n",
    "#Parch: Lo eliminaremos para tener una peor rendimiento para saber como optimizar el modelo en la siguiente sección\n",
    "\n",
    "#Entonces nuestros datos de entrada “X” tendrán los siguientes valores:\n",
    "#Sex: Genero de la persona\n",
    "#Age: Edad de la persona\n",
    "#SibSp: la cantidad de hermanos o esposa\n",
    "#Fare: El impuesto que pagaron para embarcarse\n",
    "#Embarked: Es la clase en donde se embarco la persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8364, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['country', 'year', 'sex', 'age', 'suicides_no', 'population','suicides/100k pop','country-year','HDI for year','generation'])\n",
    "df.head()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sex          age  suicides_no  population  suicides/100k pop\n",
      "72       male  25-34 years           13      232900               5.58\n",
      "73       male  55-74 years            9      178000               5.06\n",
      "74     female    75+ years            2       40800               4.90\n",
      "75     female  15-24 years           13      283500               4.59\n",
      "76       male  15-24 years           11      241200               4.56\n",
      "...       ...          ...          ...         ...                ...\n",
      "27815  female  35-54 years          107     3620833               2.96\n",
      "27816  female    75+ years            9      348465               2.58\n",
      "27817    male   5-14 years           60     2762158               2.17\n",
      "27818  female   5-14 years           44     2631600               1.67\n",
      "27819  female  55-74 years           21     1438935               1.46\n",
      "\n",
      "[8364 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seleccionamos las variables escogidas\n",
    "\n",
    "Xsubset = df[['sex', 'age', 'suicides_no', 'population', 'suicides/100k pop']]\n",
    "#Xsubset = df[['Pclass', 'Sex', 'Embarked']]\n",
    "#Xsubset.fillna(0)\n",
    "\n",
    "#para separar nuestra variable dependiente de la independiente, haremos lo siguiente:\n",
    "\n",
    "y = df.generation.values\n",
    "print(Xsubset)\n",
    "type(Xsubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sex  age  suicides_no  population  suicides/100k pop\n",
      "72       male    1           13      232900               5.58\n",
      "73       male    4            9      178000               5.06\n",
      "74     female    5            2       40800               4.90\n",
      "75     female    0           13      283500               4.59\n",
      "76       male    0           11      241200               4.56\n",
      "...       ...  ...          ...         ...                ...\n",
      "27815  female    2          107     3620833               2.96\n",
      "27816  female    5            9      348465               2.58\n",
      "27817    male    3           60     2762158               2.17\n",
      "27818  female    3           44     2631600               1.67\n",
      "27819  female    4           21     1438935               1.46\n",
      "\n",
      "[8364 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Anaconda3\\envs\\AmbientesRedesNeuronales\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "# Sample input labels\n",
    "      \n",
    "input_labels = Xsubset.iloc[:,1]\n",
    "# Create label encoder and fit the labels\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(input_labels)\n",
    "\n",
    "# Print the mapping \n",
    "#print(\"\\nLabel mapping:\")\n",
    "#for i, item in enumerate(encoder.classes_):\n",
    "#    print(item, '-->', i)\n",
    "    \n",
    "# Encode a set of labels using the encoder\n",
    "test_labels =  Xsubset.iloc[:,1]\n",
    "encoded_values = encoder.transform(test_labels)\n",
    "#print(\"\\nLabels =\", test_labels)\n",
    "#print(\"Encoded values =\", list(encoded_values))\n",
    "Xsubset.iloc[:,1] = list(encoded_values)\n",
    "print(Xsubset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre procesamiento\n",
    "\n",
    "#Generalmente cuando entrenamos modelos de “Deep Learning” antes de pasar los datos, todos los valores numéricos hay que \n",
    "#normalizarlos, porque si tenemos valores muy altos y muy bajos en la misma tabla, al pasar por la función de activación, \n",
    "#dependiendo cual sea, no lanzará resultados óptimos, por lo que se necesita un escalado, para que los valores queden a \n",
    "#la misma escala, usaremos el StandarScaler, que es para valores, que tienen mucha diferencia entre ambos.\n",
    "\n",
    "#Embarked, Sex, y PClass, son valores categóricos nominales, es decir, son valores que pertenecen a categorías, por ejemplo,\n",
    "#PClass representa la clase en que estaban embarcados los pasajeros, sin embargo, los valores son nominales puesto que un\n",
    "#pasajero de primera clase no representa mas que uno de segunda, a efectos prácticos, a nivel matemático, si lo tratamos como\n",
    "#un numero no suma ni resta. Podemos tratarlo como una categoría, porque tenemos una cantidad fija de clases\n",
    "#a lo largo de los datos. Lo mismo pasa con Embarked.\n",
    "\n",
    "#Por ser valores categóricos nominales no podemos dar las categorías directamente, tenemos que realizar el proceso de Coding\n",
    "#es decir, pasar las categorías en formato (one_hot), que consiste en poner tantos ceros como categorías, y para representar\n",
    "#un valor se coloca un uno en la posición del valor, ejemplo:\n",
    "\n",
    "#3 colores: azul, rojo, verde. la primera categoría corresponde a tantos ceros como tantas categorías que se tienen \n",
    "#azul [1,0,0]\n",
    "#verde es el tercero, por lo que para representar ese valor irá un uno en esa posición\n",
    "#verde = [0,0,1]\n",
    "\n",
    "#Documentación ColumnTransformer: \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "\n",
    "#Creamos un Pipeline (un formateador o canal) de pre procesamiento con sk_learn, usaremos la función make_column_transformer\n",
    "#Link para aprender más: https://scikit-learn.org/stable/modules/compose.html\n",
    "\n",
    "#Los transformadores generalmente se combinan con clasificadores, regresores u otros estimadores para construir un \n",
    "#estimador compuesto. La herramienta más común es un Pipeline (tubería).\n",
    "\n",
    "#Otro enlace con ejemplo: https://medium.com/vickdata/easier-machine-learning-with-the-new-column-transformer-from-scikit-learn-c2268ea9564c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(8364, 6)\n",
      "['age', 'suicides_no', 'population', 'suicides/100k pop', 'sex_female', 'sex_male']\n",
      "       age  suicides_no  population  suicides/100k pop  sex_female  sex_male\n",
      "0 -0.87831    -0.283604   -0.407927          -0.369336         0.0       1.0\n",
      "1  0.87831    -0.289478   -0.421758          -0.399288         0.0       1.0\n",
      "2  1.46385    -0.299758   -0.456321          -0.408504         1.0       0.0\n",
      "3 -1.46385    -0.283604   -0.395180          -0.426361         1.0       0.0\n",
      "4 -1.46385    -0.286541   -0.405836          -0.428089         0.0       1.0\n"
     ]
    }
   ],
   "source": [
    "#PRIMERA FORMA DE PREPROCESAR\n",
    "Xsubset = Xsubset.dropna(subset=['age','suicides_no','population','suicides/100k pop','sex'])\n",
    "\n",
    "preprocesador1 = make_column_transformer(\n",
    "    (StandardScaler(),['age','suicides_no','population','suicides/100k pop']),\n",
    "    (OneHotEncoder(),['sex']))\n",
    "\n",
    "X = preprocesador1.fit_transform(Xsubset)\n",
    "print(X.shape[1])\n",
    "print(X.shape)\n",
    "\n",
    "#print(X)\n",
    "\n",
    "#print(preprocesador1)\n",
    "categorical_features = ['sex']\n",
    "cnamesDataset1 = ['age','suicides_no', 'population', 'suicides/100k pop']\n",
    "cnamesDataset2 = preprocesador1.transformers_[1][1].get_feature_names(categorical_features)\n",
    "\n",
    "#print(cnamesDataset2)\n",
    "cnamesDataset1.extend(cnamesDataset2)\n",
    "print(cnamesDataset1)\n",
    "\n",
    "DatasetPreprocesado = pd.DataFrame(data=X,columns=cnamesDataset1)\n",
    "print(DatasetPreprocesado.head())\n",
    "\n",
    "DatasetPreprocesado.to_csv(\"MasterProcesado3.csv\", sep=\",\",index = False) #sep es el separado, por defector es \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  suicides_no  population  suicides/100k pop  sex_female  sex_male\n",
      "0 -0.87831    -0.283604   -0.407927          -0.369336         0.0       1.0\n",
      "1  0.87831    -0.289478   -0.421758          -0.399288         0.0       1.0\n",
      "2  1.46385    -0.299758   -0.456321          -0.408504         1.0       0.0\n",
      "3 -1.46385    -0.283604   -0.395180          -0.426361         1.0       0.0\n",
      "4 -1.46385    -0.286541   -0.405836          -0.428089         0.0       1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extendencoded_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-d8c6fce7ca4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDatasetPreprocesado\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mDatasetPreprocesado\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextendencoded_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'extendencoded_values' is not defined"
     ]
    }
   ],
   "source": [
    "print(DatasetPreprocesado.head())\n",
    "DatasetPreprocesado.extendencoded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "#Ahora preparamos el perceptron. Importamos las neuronas simples y el modelo secuencial\n",
    "#Modelo secuencial quiere decir que agregaremos capas y se conectarán de manera automática, \n",
    "#Dense es la librería de neuronas simples.\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "print('Librerías importadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,529\n",
      "Trainable params: 2,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 84.71%\n",
      "Guardando Red Neuronal en Archivo\n",
      "Red Neuronal Grabada en Archivo\n",
      "Red Neuronal Cargada desde Archivo\n",
      "acc: 84.71%\n"
     ]
    }
   ],
   "source": [
    "#Construcción del Modelo o Arquitectura de Redes Neoronales\n",
    "model = Sequential()\n",
    "\n",
    "#La primera capa Dense recibe el numero de variables, que es la segunda dimensión de la matriz X, esto es X_train.shape[1]\n",
    "#La primera capa tiene 32 neuronas. La función de activación es la función rectificadora.\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "#La segunda capa tiene 64 neuronas. La función de activación es la función rectificadora.\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#La capa de salida tiene 1 neurona. La capa de salida debe tener la misma dimensión como de cantidad de salidas queremos,\n",
    "#por ejemplo, en este caso la salida \"Survived\" solo requiere 0 y 1. Puesto que 0 o 1 ocupan solo un valor dentro de cada dato,\n",
    "#entonces 1 neurona es suficiente. La función de activación es sigmoide para clasificación por probabilidad.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Como tenemos dos posibles salidas \"0 o 1\", vamos a escoger que el error lo trate como una clasificación binaria, \n",
    "#el optimizador será nuestra función derivada que nos ayudará a determinar hacia donde mover los pesos.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc']) #ADADELTA: An Adaptive Learning Rate Method\n",
    "\n",
    "#imprimir arquitectura de la red\n",
    "model.summary()\n",
    "\n",
    "#Entrenamiento: \n",
    "\n",
    "#Entrenaremos por 100 epochs, el batch_size es un argumento importante, porque representa cada cuántos datos va a actualizar\n",
    "#los pesos. Este es el método del gradiente descendiente estocástico que hace el proceso más eficiente y preciso.\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0)\n",
    "score = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "\n",
    "#mostrar pesos de la red\n",
    "#print(model.get_weights())\n",
    "\n",
    "#Guardar pesos y la arquitectura de la red en un archivo \n",
    "\n",
    "print(\"Guardando Red Neuronal en Archivo\")  \n",
    "# serializar modelo a JSON\n",
    "\n",
    "# Guardar los Pesos (weights)\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Guardar la Arquitectura del modelo\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "print(\"Red Neuronal Grabada en Archivo\")    \n",
    "    \n",
    "# Cargar la Arquitectura desde el archivo JSON\n",
    "with open('model_architecture.json', 'r') as f:\n",
    "    model2 = model_from_json(f.read())\n",
    "\n",
    "# Cargar Pesos (weights) en el nuevo modelo\n",
    "model2.load_weights('model_weights.h5')  \n",
    "\n",
    "print(\"Red Neuronal Cargada desde Archivo\") \n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc']) #ADADELTA: An Adaptive Learning Rate Method\n",
    "score = model2.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model2.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: [[1]]\n",
      "Predicción: [[0]]\n"
     ]
    }
   ],
   "source": [
    "#Predicciones con nuevos datos\n",
    "\n",
    "#La siguiente función tomará los datos de entrada, los va a integrar a una dataframe, los va a preprocesar, y \n",
    "#retornará una predicción con la salida \"0\" o \"1\", es decir, \"sobrevivió\" o \"no sobrevivió\"\n",
    "def predict(Pclass=1, Sex='female', Age=60 ,Fare=0, Embarked='C'):\n",
    "    cnames = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
    "    data = [[Pclass, Sex, Age, Fare, Embarked]]\n",
    "    my_X = pd.DataFrame(data=data, columns=cnames)\n",
    "    my_X = preprocesador.transform(my_X)\n",
    "    return model.predict_classes(my_X)\n",
    "\n",
    "print('Predicción:',predict(Age=32 ,Fare=9))\n",
    "print('Predicción:',predict(Pclass=1, Sex='female', Age=60 ,Fare=0, Embarked='C'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizacion y fine tuning\n",
    "\n",
    "#Para darnos mejor idea de cuanto es nuestro error, realizaremos un proceso llamado \"cross_validation\", este proceso entrena\n",
    "#la cantidad de veces que definamos y devuelve la métrica indicada para todos los pasos, esto es mas que nada porque en cada\n",
    "#epoch la pérdida suele variar, entonces esto nos dará la precisión calculando la media de todas estas precisiones.\n",
    "\n",
    "#El proceso de Optimizacion consiste en reducir el error. Es decir, buscamos que la precision sea más alta. Mientras\n",
    "#mayor es el accuracy será mejor el modelo de red neuronal para este problema.\n",
    "\n",
    "#El proceso de \"Fine Tunning\" consiste en buscar posibles errores, y combinaciones de parámetros que puedan mejorar el modelo\n",
    "#Este proceso consume mucha memoria RAM, por lo tanto es recomendable usar alguna nube con mejores recursos a los locales, como\n",
    "#por ejemplo: Google Colab. \n",
    "#Enlaces: \n",
    "#https://colab.research.google.com/notebooks/welcome.ipynb#recent=true\n",
    "#https://colab.research.google.com/notebooks/\n",
    "\n",
    "#Estrategia de Optimización: \n",
    "#1.Compilación, \n",
    "#2.Densidad de las capas de neuronas, y \n",
    "#3.Agregar una cantidad de Dropout.\n",
    "\n",
    "#Dropout: basicamente lo que hace es apagar neuronas al azar con el fin de que las neuronas no se vuelvan tan dependientes de\n",
    "#los datos, es decir que se entrenen mejor para evitar el overfitting, para ello importaremos nuestra capa de dropout.\n",
    "#La capa dropout recibe como parámetro un numero entre 0 y 1 que representa el porcentaje de neuronas que va a desactivar en \n",
    "#esa capa, por el momento quedará en 0.1, luego optimizaremos ese valor.\n",
    "\n",
    "#Con el modelo \"GridSearchCV\" podremos optimizar todos los parámetros\n",
    "\n",
    "#importamos el algoritmo cross validator, y un wrapper que permitirá usar modelos de keras con scikit learn\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision media:  0.5481203019618988\n"
     ]
    }
   ],
   "source": [
    "#Evaluación del modelo original: Esto puede tardar unos 30 segundos.\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "#El modelo se pasa como parámetro\n",
    "estimator = KerasClassifier(build_fn=build_model, epochs=100, batch_size=64) \n",
    "#cv es la cantidad de veces de entrenamiento del modelo\n",
    "#n_jobs es para ocupar mas de un procesador. El parámetro -1 indica que queremos utilizar todos los procesadores disponibles\n",
    "accuracies=cross_val_score(estimator, X_train, y_train, cv=10, n_jobs=-1)\n",
    "mean_acc=accuracies.mean()\n",
    "print('Precision media: ', mean_acc)\n",
    "##Como resultado al proceso de entrenamiento, tenemos un accuracy en promedio aprox. de: 0.4971. Intentaremos mejorarlo.\n",
    "#Simplemente con optimizer \"adam\", el resultado aprox. es: 0.798. Pruebenlo. Cambien optimizer='adadelta' a optimizer='adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Compilación: Prueba de mejores parámetros batch_size, epochs y optimizer\n",
    "#Esto recomiendo probarlo con Google Colab, puesto que se necesita 16GB en RAM y puede llegar a tardar unos 30min.\n",
    "\n",
    "\n",
    "def build_model(optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#parámetros que queremos probar, y sus valores \n",
    "#probaremos con batch_size, epochs, y optimizador, con el fin de encontrar la mejor combinación entre estos tres parámetros.\n",
    "parameters = parameters = {'batch_size': [16,32],\n",
    "             'epochs':[100,500],\n",
    "             'optimizer': ['adadelta', 'rmsprop']}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "#Ahora no le pasamos los parámetros al KerasClasifier, porque se los pasaremos a través de GridSearchCV\n",
    "#el argumento verbose=0 es para que no muestre salida, si lo dejamos en cero, no mostrará la barra de progreso del entrenamiento\n",
    "#GridSearchCV: recibe como parámetros nuestro modelo, nuestros parámetros, la medida sobre la que queremos comparar, y la \n",
    "#cantidad de veces que lo entrenará para sacar la media de accuracy.\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n",
    "print(grid_search.best_params_)\n",
    "#Un ejemplo de resultados es: {'batch_size': 16, 'epochs': 100, 'optimizer': 'rmsprop'}\n",
    "#Esto indica que el optimizador \"adadelta\" no es adecuado. Y es que este optimizador NO sirve para este tipo de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Densidad de las capas de neuronas\n",
    "#Notemos que se incluyen los mejores parámetros del paso de optimización anterior (batch_size, epochs y optimizer)\n",
    "#Esto recomiendo probarlo con Google Colab, puesto que se necesita 16GB en RAM y puede llegar a tardar unos 30min.\n",
    "def build_model(l1, l2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(l1, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(l2, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "parameters = parameters = {'l1':[16,32,64,128,256],\n",
    "                           'l2':[16,23,64,128,256]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=100)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "#Resultados: {'l1': 32, 'l2': 16}\n",
    "#Los resultados indican que hubo un error en la red original, las capas van desde la más densa, a la menos densa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Proceso con dropouts: apagar un porcentaje de neuronas al azar con el fin de que las neuronas no se vuelvan tan \n",
    "#dependientes de los datos.\n",
    "##Esto recomiendo probarlo con Google Colab, puesto que se necesita 16GB en RAM y puede llegar a tardar unos 30min.\n",
    "def build_model(d1, d2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(d1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(d2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "parameters = parameters = {'d1':[0.1,0.2,0.3],\n",
    "                            'd2':[0.1,0.2,0.3]}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=100)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "#Resultados: {'d1':0.2, 'd2':0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UPS\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision media:  0.8067355871200561\n"
     ]
    }
   ],
   "source": [
    "#Evaluación del Modelo Final\n",
    "\n",
    "#Finalmente, veamos como mejoró nuestro modelo, vamos a repetir el proceso de la validación cruzada.\n",
    "#Esto puede probarse localmente. Ya con los mejores parámetros evaluamos la red neuronal. Puede tardar un minuto.\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=build_model, verbose=0, batch_size=16, epochs=100)\n",
    "accuracies = cross_val_score(estimator, X_train, y_train, cv=10, n_jobs=-1)\n",
    "mean_acc = accuracies.mean()\n",
    "variance_acc = accuracies.std()\n",
    "print('Precision media: ', mean_acc)\n",
    "\n",
    "#Y el resultado es:\n",
    "#Precision media aprox.: 0.8067\n",
    "#Pasamos de un 50% precisión a un 80% de precisión, por lo que se recomienda: \n",
    "#hacer siempre el proceso de fine tunning, porque ayudará a crear modelos correctos en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
